{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, sum, avg, year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/23 20:00:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"spark_tp3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Escreva um programa em Python utilizando o PySpark para ler um arquivo CSV chamado vendas.csv que contém as colunas id_venda, data_venda, produto, quantidade e preco_unitario. O programa deve carregar os dados em um DataFrame do Spark e exibir as primeiras 10 linhas do arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|\n",
      "+--------+----------+---------+----------+--------------+\n",
      "|       1|2024-09-09|Produto B|        23|        184.16|\n",
      "|       2|2024-04-22|Produto F|        38|        473.41|\n",
      "|       3|2024-04-08|Produto E|        38|        495.86|\n",
      "|       4|2024-04-07|Produto A|         8|        123.64|\n",
      "|       5|2024-09-02|Produto D|        83|        334.16|\n",
      "|       6|2024-09-25|Produto C|        71|         96.78|\n",
      "|       7|2024-08-09|Produto C|        16|        473.28|\n",
      "|       8|2025-02-07|Produto G|        48|         429.7|\n",
      "|       9|2024-10-20|Produto G|        10|        492.97|\n",
      "|      10|2024-07-08|Produto G|        47|        300.12|\n",
      "+--------+----------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.read.csv('vendas.csv',header=True)\n",
    "\n",
    "df_1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Usando o PySpark, escreva um programa que leia um arquivo CSV chamado vendas.csv, e filtre todas as vendas que ocorreram no mês de Janeiro de 2025. Exiba as primeiras 10 linhas do DataFrame filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_venda: string (nullable = true)\n",
      " |-- data_venda: string (nullable = true)\n",
      " |-- produto: string (nullable = true)\n",
      " |-- quantidade: string (nullable = true)\n",
      " |-- preco_unitario: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## verificando os tipos das colunas\n",
    "\n",
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como os dados são string, irei pegar pelos 4 primeiros digitos = 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|\n",
      "+--------+----------+---------+----------+--------------+\n",
      "|       8|2025-02-07|Produto G|        48|         429.7|\n",
      "|      17|2025-01-18|Produto E|        57|         235.0|\n",
      "|      27|2025-02-22|Produto F|        40|        155.38|\n",
      "|      28|2025-01-14|Produto A|        51|        233.21|\n",
      "|      34|2025-03-17|Produto G|        13|        463.92|\n",
      "|      47|2025-03-18|Produto E|         1|         48.48|\n",
      "|      54|2025-01-30|Produto A|        87|        338.27|\n",
      "|      59|2025-03-28|Produto F|        57|        175.72|\n",
      "|      60|2025-02-08|Produto A|         2|         99.56|\n",
      "|      65|2025-03-22|Produto B|        50|        386.75|\n",
      "+--------+----------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filtrando os dados de 2025\n",
    "\n",
    "df_2 = spark.read.csv('vendas.csv',header=True)\n",
    "\n",
    "df_2025 = df_2.filter(col('data_venda').substr(1,4) == '2025')\n",
    "\n",
    "df_2025.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Escreva um programa em Python utilizando o PySpark para calcular o valor total de vendas de cada produto. O DataFrame vendas.csv possui as colunas produto, quantidade e preco_unitario. O programa deve calcular o valor total para cada produto e exibir o resultado em ordem decrescente de valor total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|\n",
      "+--------+----------+---------+----------+--------------+\n",
      "|       1|2024-09-09|Produto B|        23|        184.16|\n",
      "|       2|2024-04-22|Produto F|        38|        473.41|\n",
      "|       3|2024-04-08|Produto E|        38|        495.86|\n",
      "|       4|2024-04-07|Produto A|         8|        123.64|\n",
      "|       5|2024-09-02|Produto D|        83|        334.16|\n",
      "+--------+----------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## antes de somar as colunas, vou precisar transformar elas \n",
    "# pro tipo int e double, pra conseguir efetuar a agregação\n",
    "df_3 = spark.read.csv('vendas.csv',header=True)\n",
    "\n",
    "df_3 = df_3.withColumn('quantidade', col('quantidade').cast('int'))\n",
    "df_3 = df_3.withColumn('preco_unitario', col('preco_unitario').cast('double'))\n",
    "\n",
    "df_3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  produto|        total_vendas|\n",
      "+---------+--------------------+\n",
      "|Produto D|1.8436860027300012E9|\n",
      "|Produto F|1.8404994087499998E9|\n",
      "|Produto B|1.8393801736199982E9|\n",
      "|Produto E|     1.83761856515E9|\n",
      "|Produto C|1.8374572282700026E9|\n",
      "|Produto G|1.8372656499099991E9|\n",
      "|Produto A| 1.830170649099999E9|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg = df_3.groupBy('produto').agg(\n",
    "    sum(col('quantidade')*col('preco_unitario')).alias('total_vendas')).orderBy(col('total_vendas').desc())\n",
    "\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Escreva um programa em Python utilizando o PySpark para ler um arquivo Parquet chamado clientes.parquet, filtrar todos os clientes que residem na cidade \"São Paulo\", e exibir os 5 primeiros registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sem arquivo clientes.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Escreva um programa em Python que, usando o PySpark, agrupe as vendas por produto e calcule o total de vendas e a média de quantidade vendida de cada produto. Exiba o resultado ordenado pela média de quantidade vendida, do maior para o menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|       valor_total|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|       1|2024-09-09|Produto B|        23|        184.16|           4235.68|\n",
      "|       2|2024-04-22|Produto F|        38|        473.41|          17989.58|\n",
      "|       3|2024-04-08|Produto E|        38|        495.86|          18842.68|\n",
      "|       4|2024-04-07|Produto A|         8|        123.64|            989.12|\n",
      "|       5|2024-09-02|Produto D|        83|        334.16|27735.280000000002|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## como a partir de agora iremos utilizar um unico dataframe, \n",
    "# sem a necessidade de leitura novamente, irei criar uma coluna com o valor total de cada venda\n",
    "\n",
    "df = spark.read.csv('vendas.csv',header=True)\n",
    "\n",
    "df = df.withColumn(\"quantidade\", col(\"quantidade\").cast(\"int\"))\n",
    "df = df.withColumn(\"preco_unitario\", col(\"preco_unitario\").cast(\"double\"))\n",
    "df = df.withColumn(\"valor_total\", col(\"quantidade\") * col(\"preco_unitario\"))\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|  produto|        total_vendas|  media_quantidade|\n",
      "+---------+--------------------+------------------+\n",
      "|Produto A| 1.830170649099999E9| 50.52657795638489|\n",
      "|Produto C|1.8374572282700026E9| 50.52634419152276|\n",
      "|Produto B|1.8393801736199982E9| 50.52142712260357|\n",
      "|Produto D|1.8436860027300012E9|50.493011755338095|\n",
      "|Produto G|1.8372656499099991E9|50.440146582652055|\n",
      "|Produto F|1.8404994087499998E9| 50.42841243175136|\n",
      "|Produto E|     1.83761856515E9| 50.38592699831449|\n",
      "+---------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_5 = df.groupBy('produto') \\\n",
    "    .agg(\n",
    "        sum('valor_total').alias('total_vendas'),\n",
    "        avg('quantidade').alias('media_quantidade')\n",
    "        ).orderBy(col('media_quantidade').desc())\n",
    "\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Escreva um programa em Python que crie uma UDF (função definida pelo usuário) no PySpark para calcular o valor total de cada venda (quantidade * preco_unitario) e adicione essa coluna ao DataFrame. Exiba as primeiras 10 linhas com a nova coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcao aborda desde a multiplicacao do valor, até a criação em uma nova coluna\n",
    "def add_column_total(dataframe: DataFrame, nome_coluna1: str, nome_coluna2: str, nome_coluna_total: str) -> DataFrame:\n",
    "    dataframe = dataframe.withColumn(nome_coluna_total, col(nome_coluna1) * col(nome_coluna2))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+------------------+------------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|       valor_total|   ex6_valor_total|\n",
      "+--------+----------+---------+----------+--------------+------------------+------------------+\n",
      "|       1|2024-09-09|Produto B|        23|        184.16|           4235.68|           4235.68|\n",
      "|       2|2024-04-22|Produto F|        38|        473.41|          17989.58|          17989.58|\n",
      "|       3|2024-04-08|Produto E|        38|        495.86|          18842.68|          18842.68|\n",
      "|       4|2024-04-07|Produto A|         8|        123.64|            989.12|            989.12|\n",
      "|       5|2024-09-02|Produto D|        83|        334.16|27735.280000000002|27735.280000000002|\n",
      "|       6|2024-09-25|Produto C|        71|         96.78|           6871.38|           6871.38|\n",
      "|       7|2024-08-09|Produto C|        16|        473.28|           7572.48|           7572.48|\n",
      "|       8|2025-02-07|Produto G|        48|         429.7|           20625.6|           20625.6|\n",
      "|       9|2024-10-20|Produto G|        10|        492.97| 4929.700000000001| 4929.700000000001|\n",
      "|      10|2024-07-08|Produto G|        47|        300.12|          14105.64|          14105.64|\n",
      "+--------+----------+---------+----------+--------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_6 = add_column_total(df, \"quantidade\", \"preco_unitario\", \"ex6_valor_total\")\n",
    "\n",
    "df_6.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Escreva um programa em Python utilizando o PySpark para filtrar todas as vendas realizadas após o dia 1º de Fevereiro de 2025, onde a quantidade vendida foi superior a 10 unidades. Exiba as primeiras 10 linhas do DataFrame filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_venda: string (nullable = true)\n",
      " |-- data_venda: date (nullable = true)\n",
      " |-- produto: string (nullable = true)\n",
      " |-- quantidade: integer (nullable = true)\n",
      " |-- preco_unitario: double (nullable = true)\n",
      " |-- valor_total: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('data_venda', col('data_venda').cast('date'))\n",
    "\n",
    "##usando print schema para confirmar que mudou o tipo para date\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|       valor_total|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|       8|2025-02-07|Produto G|        48|         429.7|           20625.6|\n",
      "|      27|2025-02-22|Produto F|        40|        155.38|            6215.2|\n",
      "|      34|2025-03-17|Produto G|        13|        463.92|           6030.96|\n",
      "|      59|2025-03-28|Produto F|        57|        175.72|10016.039999999999|\n",
      "|      65|2025-03-22|Produto B|        50|        386.75|           19337.5|\n",
      "|      77|2025-03-06|Produto D|        54|        211.43|11417.220000000001|\n",
      "|      82|2025-02-01|Produto C|        78|        392.68|          30629.04|\n",
      "|      89|2025-02-15|Produto D|        97|        179.44|          17405.68|\n",
      "|      90|2025-02-16|Produto A|        64|        191.52|          12257.28|\n",
      "|      95|2025-03-10|Produto B|        51|        224.82|          11465.82|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##filtrando dados > 01/02/2025 com mais que 10 unidades\n",
    "\n",
    "df_7 = df.filter('date(data_venda) >= \"2025-02-01\" and quantidade > 10')\n",
    "\n",
    "df_7.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Escreva um programa em Python utilizando o PySpark para ordenar as vendas pelo valor total (quantidade * preço_unitário) em ordem decrescente. Exiba as primeiras 10 linhas do DataFrame ordenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+-----------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|valor_total|\n",
      "+--------+----------+---------+----------+--------------+-----------+\n",
      "|  351434|2024-02-16|Produto E|       100|         499.9|    49990.0|\n",
      "|  771838|2024-11-06|Produto C|       100|        499.84|    49984.0|\n",
      "|  468812|2024-02-26|Produto A|       100|        499.73|    49973.0|\n",
      "|  917273|2024-10-11|Produto B|       100|        499.67|    49967.0|\n",
      "|  370230|2024-02-05|Produto D|       100|        499.45|    49945.0|\n",
      "|  206785|2024-10-25|Produto A|       100|         499.2|    49920.0|\n",
      "|  662919|2024-09-13|Produto F|       100|        499.14|    49914.0|\n",
      "|  829676|2024-01-26|Produto F|       100|        499.12|    49912.0|\n",
      "|  962180|2024-02-23|Produto E|       100|        499.02|    49902.0|\n",
      "|  777089|2025-01-25|Produto G|       100|        498.94|    49894.0|\n",
      "+--------+----------+---------+----------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_8 = df.orderBy(col('valor_total').desc())\n",
    "\n",
    "df_8.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Escreva um programa em Python utilizando o PySpark para substituir os valores nulos na coluna preco_unitario por 0. Exiba as primeiras 10 linhas do DataFrame após o tratamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O total de valores nulos no dataframe é: 0\n"
     ]
    }
   ],
   "source": [
    "#contando a quantidade de valores nulos antes da transformação\n",
    "df_null = df.filter(col(\"preco_unitario\").isNull()).count()\n",
    "\n",
    "print(f'O total de valores nulos no dataframe é: {df_null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por mais que não tenha valores nulos nessa coluna, irei deixar abaixo um exemplo de como eu faria para poder filtrar os valores nulos nela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|       valor_total|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|       1|2024-09-09|Produto B|        23|        184.16|           4235.68|\n",
      "|       2|2024-04-22|Produto F|        38|        473.41|          17989.58|\n",
      "|       3|2024-04-08|Produto E|        38|        495.86|          18842.68|\n",
      "|       4|2024-04-07|Produto A|         8|        123.64|            989.12|\n",
      "|       5|2024-09-02|Produto D|        83|        334.16|27735.280000000002|\n",
      "|       6|2024-09-25|Produto C|        71|         96.78|           6871.38|\n",
      "|       7|2024-08-09|Produto C|        16|        473.28|           7572.48|\n",
      "|       8|2025-02-07|Produto G|        48|         429.7|           20625.6|\n",
      "|       9|2024-10-20|Produto G|        10|        492.97| 4929.700000000001|\n",
      "|      10|2024-07-08|Produto G|        47|        300.12|          14105.64|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_9 = df.na.fill({'preco_unitario': 0})\n",
    "\n",
    "df_9.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Escreva um programa em Python utilizando o PySpark para salvar os dados filtrados de vendas ocorridas após 1º de Janeiro de 2025 em um arquivo Parquet chamado vendas_filtradas.parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 20:37:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/23 20:37:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/23 20:37:46 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/03/23 20:37:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/03/23 20:37:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    }
   ],
   "source": [
    "df_10 = df.filter('data_venda > \"2025-01-01\"')\n",
    "\n",
    "df_10.write.parquet('vendas_filtradas.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir do método acima, pela arquitetura de processamento do Spark, ele dividiu o arquivo em várias partes. \\\n",
    "Abaixo, criarei um arquivo unico com todo o material dentro dele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10.coalesce(1).write.parquet('vendas_filtradas_unico.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|id_venda|data_venda|  produto|quantidade|preco_unitario|       valor_total|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "|       8|2025-02-07|Produto G|        48|         429.7|           20625.6|\n",
      "|      17|2025-01-18|Produto E|        57|         235.0|           13395.0|\n",
      "|      27|2025-02-22|Produto F|        40|        155.38|            6215.2|\n",
      "|      28|2025-01-14|Produto A|        51|        233.21|11893.710000000001|\n",
      "|      34|2025-03-17|Produto G|        13|        463.92|           6030.96|\n",
      "|      47|2025-03-18|Produto E|         1|         48.48|             48.48|\n",
      "|      54|2025-01-30|Produto A|        87|        338.27|29429.489999999998|\n",
      "|      59|2025-03-28|Produto F|        57|        175.72|10016.039999999999|\n",
      "|      60|2025-02-08|Produto A|         2|         99.56|            199.12|\n",
      "|      65|2025-03-22|Produto B|        50|        386.75|           19337.5|\n",
      "+--------+----------+---------+----------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## teste se funcionou o save do arquivo\n",
    "\n",
    "df_parquet_teste = spark.read.parquet('vendas_filtradas_unico.parquet',header=True)\n",
    "\n",
    "df_parquet_teste.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Utilizando o arquivo csv: \\\n",
    "Carregue o arquivo em um RDD. \\\n",
    "Filtre todas as vendas realizadas entre os meses 3 e 8 de 2024. \\\n",
    "Calcular o valor total dessas vendas por produto. \\\n",
    "Identifique os 5 produtos com o maior valor total de vendas e exibi-los. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abaixo, podemos analisar os 5 produtos que mais venderam entre Março e Agosto de 2024:\n",
      "+---------+------------------------+\n",
      "|  produto|vendas_totais_acumuladas|\n",
      "+---------+------------------------+\n",
      "|Produto G|       4.9354021711506E9|\n",
      "|Produto D|     4.920936952156597E9|\n",
      "|Produto F|     4.917284333100503E9|\n",
      "|Produto A|       4.9008267901676E9|\n",
      "|Produto E|     4.892630117801302E9|\n",
      "+---------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_11 = spark.read.csv('vendas.csv', header=True)\n",
    "\n",
    "df_11 = df_11.withColumn('quantidade',col('quantidade').cast('int'))\n",
    "df_11 = df_11.withColumn('preco_unitario',col('preco_unitario').cast('double'))\n",
    "df_11 = df_11.withColumn('data_venda', col('data_venda').cast('date'))\n",
    "\n",
    "#criando coluna de valor_total\n",
    "df_11 = df_11.withColumn('valor_total',col('preco_unitario') * col('preco_unitario'))\n",
    "\n",
    "df_filtered = df_11.filter('data_venda between \"2024-03-01\" and \"2024-08-31\"')\n",
    "\n",
    "df_agg = df_filtered.groupBy('produto') \\\n",
    "                            .agg(sum(col('valor_total')) \\\n",
    "                            .alias('vendas_totais_acumuladas')) \\\n",
    "                            .orderBy(col('vendas_totais_acumuladas').desc())\n",
    "\n",
    "print('Abaixo, podemos analisar os 5 produtos que mais venderam entre Março e Agosto de 2024:')\n",
    "df_agg.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
